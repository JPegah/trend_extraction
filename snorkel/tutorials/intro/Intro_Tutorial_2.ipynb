{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Generating _and modeling_ noisy training labels\n",
    "\n",
    "In this part of the tutorial, we will write **labeling functions** which express various heuristics, patterns, and [_weak supervision_](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data.\n",
    "\n",
    "In most real-world settings, hand-labeled training data is prohibitively expensive and slow to collect. A common scenario, though, is to have access to tons of _unlabeled_ training data, and have some idea of how to label it programmatically. For example:\n",
    "\n",
    "* We may be able to think of text patterns that would indicate two people mentioned in a sentence are married, such as seeing the word \"spouse\" between the mentions.\n",
    "* We may have access to an external _knowledge base (KB)_ that lists some known pairs of married people, and can use these to heuristically label some subset of our data.\n",
    "\n",
    "Our labeling functions will capture these types of strategies. We know that these labeling functions will not be perfect, and some may be quite low-quality, so we will _model_ their accuracies with a generative model, which Snorkel will help us easily apply.\n",
    "\n",
    "This will ultimately produce a single set of **noise-aware training labels**, which we will then use to train an end extraction model in the next notebook.  For more technical details of this overall approach, see our [NIPS 2016 paper](https://arxiv.org/abs/1605.07723)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass from Parts II and III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "#Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "Trend = candidate_subclass('Trend', ['tr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a labeled _development set_\n",
    "\n",
    "In our setting here, we will use the phrase \"development set\" to refer to a _small_ set of examples (here, a subset of our training set) which we label by hand and use to help us develop and refine labeling functions. Unlike the _test set_, which we do not look at and use for final evaluation, we can inspect the development set while writing labeling functions.\n",
    "\n",
    "In our case, we already loaded existing labels for a development set (`split` 1), so we can load them again now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# print(L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Modeling a Noisy Training Set\n",
    "\n",
    "Our biggest step in the data programming pipeline is the creation - _and modeling_ - of a noisy training set.  We'll approach this in three main steps:\n",
    "\n",
    "1. **Creating labeling functions (LFs):** This is where most of our development time would actually go into if this were a real application. Labeling functions encode our heuristics and weak supervision signals to generate (noisy) labels for our training candidates.\n",
    "\n",
    "2. **Applying the LFs:** Here, we actually use them to label our candidates!\n",
    "\n",
    "3. **Training a generative model of our training set:** Here we learn a model over our LFs, learning their respective accuracies automatically. This will allow us to combine them into a single, higher-quality label set.\n",
    "\n",
    "We'll also add some detail on how to go about _developing labeling functions_ and then _debugging our model_ of them to improve performance.\n",
    "\n",
    "## 1. Creating Labeling Functions\n",
    "\n",
    "In Snorkel, our primary interface through which we provide training signal to the end extraction model we are training is by writing **labeling functions (LFs)** (as opposed to hand-labeling massive training sets).  We'll go through some examples for our spouse extraction task below.\n",
    "\n",
    "A labeling function is just a Python function that accepts a `Candidate` and returns `1` to mark the `Candidate` as true, `-1` to mark the `Candidate` as false, and `0` to abstain from labeling the `Candidate` (note that the non-binary classification setting is covered in the advanced tutorials!).\n",
    "\n",
    "In the next stages of the Snorkel pipeline, we'll train a model to learn the accuracies of the labeling functions and trewieght them accordingly, and then use them to train a downstream model. It turns out by doing this, we can get high-quality models even with lower-quality labeling functions. So they don't need to be perfect! Now on to writing some:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,get_sent_candidate_spans\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern-based LFs\n",
    "These LFs express some common sense text patterns which indicate that a person pair might be married. For example, `LF_husband_wife` looks for words in `spouses` between the person mentions, and `LF_same_last_name` checks to see if the two people have the same last name (but aren't the same whole name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "trend_lexicon = {\n",
    "            \"unknown\": [\"seasonality\",  \"consistent\", \"rebound\", \"average\", \"exceed\"\n",
    "                        \"steadily\", \"regain\", \"rebound\", \"retrace\", \"hyperdemic\", \"advance\", \"retreat\", \"up\", \"fall\", \"growth\"\n",
    "                        \"trend\", \"bounce\"],\n",
    "            \"single\": [\"peak\", \"minima\", \"maxima\", \"slip\", \"minimum\", \"maximum\", \"jump\", \"leap\", \"highest\", \"lowest\"],\n",
    "            \"linear\": [\"reduce\", \"reduction\",  \"increase\", \"rise\", \"rising\", \"grow\", \"flat\",\n",
    "                       \"drop\", \"gain\", \"downtrend\", \"plummet\", \"descend\", \"down\", \"decline\", \"downward\", \"firm\"], # I removed the word \"fall\" from the candidates\n",
    "            \"UNK\": [\"correlation\", \"teeter\", \"progression\", \"progress\", \"random\", \"polynomial\", \"parabolic\",\n",
    "                    \"hyperdemic\", \"trend\", \"exponential\", \"exponentially\"],\n",
    "            \"LIN\": [\"downfall\", \"downtrend\", \"advance\", \"uptrend\", \"linear\", \"increase\", \"decrease\", \"decline\",\n",
    "                     \"grow\", \"rise\", \"drop\", \"climbs\", \"descend\", \"decay\", \"constant\", \"slope\",\n",
    "                    \"increment\", \"decrement\", \"plummet\", \"reduce\", \"ascent\", \"descent\",\n",
    "                    \"reduce\", \"reduction\", \"increase\", \"rise\", \"rising\", \"grow\", \"flat\", \"drop\",\n",
    "                    \"gain\", \"downtrend\", \"descend\", \"decline\", \"downward\"],\n",
    "            \"SING\": [\"bump\", \"spike\", \"spiking\", \"crest\", \"jump\", \"pulse\", \"anomalous\", \"outlier\", \"gap\", \"peak\",\n",
    "                     \"valley\", \"slip\", \"bounce\", \"jump\", \"leap\"],\n",
    "            \"VAL\": [\"maximum\", \"minimum\", \"peak\", \"minima\", \"maxima\", \"average\"],\n",
    "            \"OSC-VAR\": [\"inconsistent\", \"consistent\", \"hover\", \"steadily\", \"volatile\", \"steady\", \"oscillate\", \"flux\",\n",
    "                        \"turbulence\", \"fluctuation\", \"fluctuate\", \"inflexion\", \"oscillation\", \"steady\", \"vacillate\",\n",
    "                        \"cluster\", \"variation\", \"stabilize\", \"variance\", \"disperse\", \"settle\", \"divergent\",\n",
    "                        \"consistent\"],\n",
    "            \"SEC-LIN\": [\"acceleration\", \"accelerate\", \"regain\", \"downturn\", \"rebound\", \"retreat\", \"retrace\",\n",
    "                        \"turnaround\"],\n",
    "            \"CYC\": [\"wave\", \"sinusodal\", \"seasonality\", \"cycle\", \"cyclic\", \"sine\"]\n",
    "        }\n",
    "value_kw = [\"above\", \"below\", \"level\", \"rate\", \"level\", \"percent\", \"compare\", \"between\", \"high\", \"low\", \"highest\", \"record\", \"less\"]\n",
    "\n",
    "\n",
    "\n",
    "def is_indicator(c): # c can be string of any lenght\n",
    "    if c in bi_words:\n",
    "        return True\n",
    "    if c in bi_words_src:\n",
    "        return True\n",
    "    if c in trip_words:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def poss_indicator(c):\n",
    "    if c in single_words:\n",
    "        return True\n",
    "    if c in single_src_words:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def count_indicators(sent): # assumption for now is the spacy sentence\n",
    "    num = 0\n",
    "    for i in range(len(sent) - 1):\n",
    "        bi_word = str(sent[i].text).lower() + \" \" + str(sent[i+1].text).lower()\n",
    "#         print(bi_word)\n",
    "        if is_indicator(bi_word):\n",
    "            num += 1\n",
    "#             print(bi_word)\n",
    "        if i+2 < len(sent):\n",
    "            trip_word = bi_word + \" \" + str(sent[i+2].text).lower()\n",
    "            if is_indicator(trip_word):\n",
    "                num += 1\n",
    "        if poss_indicator(str(sent[i]).lower()):\n",
    "            num += 0.5\n",
    "    return num\n",
    "    \n",
    "\n",
    "def load_word_file(name):\n",
    "    in_file = open('data/'+ name)\n",
    "    w_dict = set([])\n",
    "    for w in in_file.readlines():\n",
    "        w_dict.add(w)\n",
    "    return w_dict\n",
    "\n",
    "def in_trend_lexicon(c):\n",
    "    for kw in trend_lexicon.keys():\n",
    "        if c.lower() in trend_lexicon[kw]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_spacy_sent(s):\n",
    "    sent = nlp(get_tagged_text(s).replace('{{A}}', str(s.tr.get_span())))\n",
    "#     print(sent)\n",
    "    return sent\n",
    "    \n",
    "def get_spacy_token(s, spacy_sent, distance):\n",
    "    step = 0\n",
    "    self_token = None\n",
    "    tmp = str(s)\n",
    "    token_place = re.search(r'words=\\[.*\\]', tmp)\n",
    "    tmp = token_place[0].split(\",\")[1]\n",
    "    token_ind = int(tmp[:-1])\n",
    "    for token in spacy_sent:\n",
    "        step += 1\n",
    "        if step == token_ind + distance + 1:\n",
    "            return token\n",
    "    return None\n",
    "   \n",
    "def LF_up_down(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.lemma_).lower() in [\"up\", \"down\"]:\n",
    "        if str(s_token.pos_) == \"PART\" and str(s_token.pos_) == 'ADP':\n",
    "            if str(s_token.head.pos_) == \"VERB\":\n",
    "                phrasal_v = str(s_token.head.lemma_) + \" \" + str(s_token.lemma_)\n",
    "                print(phrasal_v)\n",
    "            #print(\"This is the phrasal verb to be evaluated \" + phrasal_v)\n",
    "                if phrasal_v in [\"move up\", \"go up\", \"fall down\", \"cut down\"]:\n",
    "                    return 1\n",
    "                return -1\n",
    "        \n",
    "        tmp = get_right_tokens(c, window=1, attrib='words', n_max=1, case_sensitive=False)\n",
    "        for x in tmp:\n",
    "            if str(x).lower() in ['from']: #, 'to']:\n",
    "                return 1\n",
    "        if str(s_token.dep_) == 'advmod':\n",
    "            return 1\n",
    "        return -1 # TODO: check if this should be 0\n",
    "    return 0\n",
    "\n",
    "def LF_decline(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.lemma_).lower() == 'decline':\n",
    "        for child in s_token.children:\n",
    "            if str(child.pos_) == 'VERB':\n",
    "                return -1\n",
    "        return 1\n",
    "    return 0 # the right label? \n",
    "\n",
    "def LF_Person_subj_obj(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.pos_) != 'VERB':\n",
    "        return 0\n",
    "    obj = None\n",
    "    subj = None\n",
    "    for token in s_token.children: # TODO: it has to be more sophisticated\n",
    "    #for token in sent:\n",
    "        if str(token.dep_) == 'dobj':\n",
    "            obj = token\n",
    "        if str(token.dep_) == 'nsubj' or str(token.dep_) == 'nsubjpass':\n",
    "            subj = token\n",
    "   \n",
    "    if obj == None and subj != None:\n",
    "        if str(subj.text).lower() in ['he', 'she', 'they', 'we']:\n",
    "            return -1\n",
    "        if str(subj.ent_type) == 'PERSON':\n",
    "            return -1\n",
    "        \n",
    "    if obj != None:\n",
    "        if str(obj.text).lower() == 'price':\n",
    "            return 1\n",
    "    if obj != None and subj != None:\n",
    "        return 1 # This is really not good enough. other factors should be considered\n",
    "    return 0 #TODO: check to see if there should be a place to return 1\n",
    "\n",
    "def LF_explicit_change(s): # from x to Y\n",
    "    sent = get_spacy_sent(s)\n",
    "    counter = 0\n",
    "    sentence_str = \"\"\n",
    "    for token in sent:\n",
    "        sentence_str += token.text + \" \"\n",
    "        if str(token.lemma_) in ['from', 'to'] and str(token.pos_) == 'ADP': # TODO: try to exclude the date range part in here\n",
    "            counter += 1\n",
    "        \n",
    "    if counter < 2:\n",
    "        return 0\n",
    "    if re.match('.* from .+ to .+', sentence_str):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# if this token is part of named entity then it is not the exact trend description\n",
    "def LF_entity(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.ent_type_) != \"\":\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_consecutives(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    prev_token = get_spacy_token(s, sent, -1)\n",
    "    next_token = get_spacy_token(s, sent, 1)\n",
    "    if prev_token != None and s_token != None and in_trend_lexicon(prev_token.lemma_) and str(s_token.lemma_) in trend_lexicon[\"linear\"]:\n",
    "        return -1\n",
    "    if next_token != None and in_trend_lexicon(next_token.lemma_):\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "    \n",
    "def LF_helping_kw(s): # words like trend, ..\n",
    "    words = [\"trend\", \"pace\", \"constant\", \"continue\"]\n",
    "    for w in s.get_parent().words:\n",
    "        if w in words:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_percent_dependant(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    for child in s_token.children:\n",
    "        if str(child.ent_type_) == 'PERCENT':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_since_during(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    for token in sent:\n",
    "        if str(token.text).lower() in ['since', 'during', 'while'] and str(token.pos_) == 'ADP':\n",
    "            for child in token.children:\n",
    "                if str(child.ent_type) == 'DATE': # better date and temporal signs to be added\n",
    "                    return 1\n",
    "                if date_signals(str(child.lemma_).lower()):\n",
    "                    return 1\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "        \n",
    "def LF_continue(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.pos_) != 'VERB':\n",
    "        return 0\n",
    "    \n",
    "    for token in sent:\n",
    "        if str(token.lemma_).lower() == 'continue' and token.head == s_token:\n",
    "#         if s_token in token.children and str(token.lemma_) == 'continue':\n",
    "            return 1\n",
    "    return 0\n",
    "  \n",
    "\n",
    "def LF_value_kw(s):\n",
    "    for w in s.get_parent().words:\n",
    "        if w in value_kw:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_extrema(s):\n",
    "    if str(s.tr.get_span()) not in ['highest', 'lowest']: # TODO: what about highest among, lowest among?\n",
    "        return 0\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    s_parent = s_token.head\n",
    "    for token in sent:\n",
    "        if str(token.ent_type) == 'DATE': # TODO: check for other the date formats\n",
    "            return 1\n",
    "        if date_signals(w):\n",
    "            return 1\n",
    "        \n",
    "    return -1\n",
    "    \n",
    "def date_signals(w):\n",
    "    kws = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "    kw_t = [\"every\", \"weekly\", \"monthly\", \"yearly\", \"daily\", \"ago\", \"until\", \"recently\", \"everyday\", \"early\", \n",
    "             \"recent\", \"annual\", \"weekday\", \"prior\", \"today\",\n",
    "            \"period\", \"hour\", \"timeline\", \"before\", \"after\", \"later\", \"second\", \"annually\",\n",
    "            \"biennial\", \"quarter\", \"earlier\", \"latest\", \"period\", \"long-term\",\"decade\", \"while\", \"since\", \"during\",\n",
    "            ]\n",
    "    if w in kws:\n",
    "        return True\n",
    "    if w in kw_t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def LF_date_sign(s): #TODO: add my own observed date patterns\n",
    "    sent = get_spacy_sent(s)\n",
    "    sentence_str = \"\"\n",
    "    kws = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "            \n",
    "    for token in sent:\n",
    "        if str(token.lemma_).lower() in kws:\n",
    "            return 1\n",
    "\n",
    "        if str(token.ent_type_) == 'DATE':\n",
    "            return 1\n",
    "    # adding my own date signs\n",
    "    return 0\n",
    "\n",
    "def LF_temporal_signs(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    kw_t = [\"every\", \"weekly\", \"monthly\", \"yearly\", \"daily\", \"ago\", \"until\", \"recently\", \"everyday\", \"early\", \n",
    "             \"recent\", \"annual\", \"weekday\", \"prior\", \"today\",\n",
    "            \"period\", \"hour\", \"timeline\", \"before\", \"after\", \"later\", \"second\", \"annually\",\n",
    "            \"biennial\", \"quarter\", \"earlier\", \"latest\", \"period\", \"long-term\",\"decade\",\n",
    "            ]\n",
    "    for token in sent:\n",
    "        if str(token.lemma_).lower() in kw_t:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "def LF_temporal_patterns(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    sentence_str = \"\"\n",
    "    for token in sent:\n",
    "        sentence_str += str(token.text).lower() + \" \"\n",
    "    for p in [\"each\", \"every\", \"last\", \"past\", \"next\", \"previous\", \"this\"]:\n",
    "        for p1 in [\"year\", \"month\", \"week\", \"day\", \"time\", \"minute\"]:\n",
    "            if re.match(p + p1 + '.*', sentence_str):\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "# def LF_adv_linear(s):\n",
    "#     sent = get_spacy_sent(s)\n",
    "#     s_token = get_spacy_token(s, sent, 0)\n",
    "#     if str(s_token.lemma_) in trend_lexicon['LIN'] or str(s_token.lemma_) in trend_lexicon['linear']:\n",
    "#         if str(s_token.pos_) != 'ADV':\n",
    "#             return -1\n",
    "#     return 0\n",
    "\n",
    "def LF_linear_tense(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.lemma_) in trend_lexicon['LIN'] or str(s_token.lemma_) in trend_lexicon['linear']:\n",
    "        if str(s_token.pos_) != 'VERB':\n",
    "            return 0\n",
    "        print (nlp.vocab.morphology.tag_map[s_token.tag_])\n",
    "        if 'Tense' not in nlp.vocab.morphology.tag_map[s_token.tag_].keys():\n",
    "            return 0\n",
    "        if nlp.vocab.morphology.tag_map[s_token.tag_]['Tense'] == 'past':\n",
    "            if 'Aspect' in nlp.vocab.morphology.tag_map[s_token.tag_].keys() and nlp.vocab.morphology.tag_map[s_token.tag_]['Aspect'] == 'perf':\n",
    "                return 1\n",
    "        if nlp.vocab.morphology.tag_map[s_token.tag_]['Tense'] == 'pres':\n",
    "            if 'Aspect' in nlp.vocab.morphology.tag_map[s_token.tag_].keys() and nlp.vocab.morphology.tag_map[s_token.tag_]['Aspect'] == 'prog':\n",
    "#                 if str(s_token.head.lemma_)  == 'by': #TODO: this is tricky\n",
    "#                     return -1\n",
    "                for ch in s_token.children:\n",
    "                    if str(ch.dep_) == 'aux' and str(ch.lemma_) == 'be':\n",
    "                        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_dependency(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.pos_) == 'VERB' and str(s_token.dep_) == 'amod':\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_cause_effect(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    s_token = get_spacy_token(s, sent, 0)\n",
    "    if str(s_token.pos_) == 'VERB': \n",
    "        if str(s_token.dep_) == 'advcl':\n",
    "            return -1\n",
    "        for child in s_token.children:\n",
    "                if str(child.dep_) == 'aux' and str(child.text) == 'to': # better date and temporal signs to be added\n",
    "                    return -1\n",
    "    return 0\n",
    "\n",
    "single_words = load_word_file('freq_words.txt')\n",
    "single_src_words = load_word_file('freq_words_src.txt')\n",
    "\n",
    "bi_words = load_word_file('freq_bi_words.txt')\n",
    "bi_words_src = load_word_file('freq_bi_words_src.txt')\n",
    "\n",
    "trip_words = load_word_file('freq_tip_words.txt')\n",
    "\n",
    "def LF_indicator_signs(s):\n",
    "    sent = get_spacy_sent(s)\n",
    "    if count_indicators(sent) >= 0.5:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distant Supervision LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to writing labeling functions that describe text pattern-based heuristics for labeling training examples, we can also write labeling functions that distantly supervise examples. Here, we'll load in a list of known spouse pairs and check to see if the candidate pair matches one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later convenience we group the labeling functions into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship, \n",
    "    LF_entity, LF_up_down, LF_consecutives, LF_helping_kw, LF_decline, LF_temporal_signs,\n",
    "#     LF_adv_linear,\n",
    "    LF_percent_dependant, \n",
    "    LF_since_during,\n",
    "    LF_dependency,\n",
    "    LF_cause_effect,\n",
    "    LF_explicit_change, LF_continue, LF_temporal_patterns, \n",
    "    LF_value_kw,  LF_extrema, LF_Person_subj_obj,  \n",
    "#     LF_up_down_change,\n",
    "    LF_date_sign, LF_linear_tense,\n",
    "    LF_indicator_signs\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Labeling Functions\n",
    "\n",
    "Above, we've written a bunch of labeling functions already, which should give you some sense about how to go about it. While writing them, we probably want to check to make sure that they at least work as intended before adding to our set. Suppose we're thinking about writing a simple LF: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def LF_wife_in_sentence(c):\n",
    "    \"\"\"A simple example of a labeling function\"\"\"\n",
    "    return 1 if 'wife' in c.get_parent().words else 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple thing we can do is quickly test it on our development set (or any other set), without saving it to the database.  This is simple to do. For example, we can easily get every candidate that this LF labels as true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labeled: 0\n"
     ]
    }
   ],
   "source": [
    "# This part is for testing the labeling function\n",
    "labeled = []\n",
    "for c in session.query(Trend).filter(Trend.split == 1).all():\n",
    "#     print(c)\n",
    "    if LF_indicator_signs(c) != 0:\n",
    "        labeled.append(c)\n",
    "        print(c)\n",
    "print(\"Number labeled:\", len(labeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily put this into the Viewer as usual (try it out!):\n",
    "```\n",
    "SentenceNgramViewer(labeled, session)\n",
    "```\n",
    "\n",
    "We also have a simple helper function for getting the empirical accuracy of a single LF with respect to the development set labels for example. This function also returns the evaluation buckets of the candidates (true positive, false positive, true negative, false negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are selected candidates to be tested\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "[Trend(Span(\"b'reduced'\", sentence=769, chars=[28,34], words=[6,6])), Trend(Span(\"b'down'\", sentence=769, chars=[110,113], words=[19,19])), Trend(Span(\"b'up'\", sentence=766, chars=[107,108], words=[20,20])), Trend(Span(\"b'average'\", sentence=735, chars=[17,23], words=[4,4])), Trend(Span(\"b'up'\", sentence=743, chars=[8,9], words=[2,2])), Trend(Span(\"b'up'\", sentence=422, chars=[90,91], words=[16,16])), Trend(Span(\"b'up'\", sentence=356, chars=[194,195], words=[31,31])), Trend(Span(\"b'fall'\", sentence=411, chars=[155,158], words=[26,26])), Trend(Span(\"b'up'\", sentence=355, chars=[335,336], words=[58,58])), Trend(Span(\"b'grow'\", sentence=732, chars=[84,87], words=[14,14])), Trend(Span(\"b'falling'\", sentence=417, chars=[97,103], words=[19,19])), Trend(Span(\"b'jump'\", sentence=413, chars=[87,90], words=[13,13])), Trend(Span(\"b'reduction'\", sentence=433, chars=[110,118], words=[19,19])), Trend(Span(\"b'jump'\", sentence=414, chars=[4,7], words=[1,1])), Trend(Span(\"b'fall'\", sentence=414, chars=[25,28], words=[6,6])), Trend(Span(\"b'down'\", sentence=440, chars=[188,191], words=[29,29])), Trend(Span(\"b'up'\", sentence=440, chars=[21,22], words=[4,4])), Trend(Span(\"b'increased'\", sentence=367, chars=[70,78], words=[12,12])), Trend(Span(\"b'growing'\", sentence=749, chars=[44,50], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=412, chars=[82,87], words=[12,12])), Trend(Span(\"b'up'\", sentence=425, chars=[114,115], words=[22,22])), Trend(Span(\"b'grew'\", sentence=418, chars=[48,51], words=[10,10])), Trend(Span(\"b'reduce'\", sentence=760, chars=[110,115], words=[23,23])), Trend(Span(\"b'advancing'\", sentence=369, chars=[64,72], words=[11,11]))]\n",
      "--------------------Test labels:-------------------\n",
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-1\n",
      "  (2, 0)\t-1\n",
      "  (3, 0)\t-1\n",
      "  (4, 0)\t-1\n",
      "  (5, 0)\t-1\n",
      "  (6, 0)\t-1\n",
      "  (7, 0)\t-1\n",
      "  (8, 0)\t-1\n",
      "  (9, 0)\t-1\n",
      "  (10, 0)\t1\n",
      "  (11, 0)\t-1\n",
      "  (12, 0)\t-1\n",
      "  (13, 0)\t-1\n",
      "  (14, 0)\t-1\n",
      "  (15, 0)\t-1\n",
      "  (16, 0)\t-1\n",
      "  (17, 0)\t-1\n",
      "  (18, 0)\t1\n",
      "  (19, 0)\t-1\n",
      "  (20, 0)\t-1\n",
      "  (21, 0)\t-1\n",
      "  (22, 0)\t-1\n",
      "  (23, 0)\t-1\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 22 | FN: 2\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b087f5885b42799aacf61c6b6a35e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SentenceNgramViewer(html='<head>\\n<style>\\nspan.candidate {\\n    background-color: rgba(255,255,0,0.3);\\n}\\n\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session,  LF_indicator_signs, split=1, annotator_name='gold')\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(fp, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database.  We'll do this using the `LabelAnnotator` class, a UDF which we will again run with `UDFRunner`.  **Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.**  We start by setting up the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the `labeler`. Note that we set a random seed for reproducibility, since some of the LFs involve random number generators. Again, this can be run in parallel, given an appropriate database like Postgres is being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/103 [00:00<00:04, 23.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/103 [00:00<00:05, 17.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n",
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/103 [00:01<00:04, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/103 [00:01<00:04, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 29/103 [00:01<00:03, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/103 [00:02<00:04, 16.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 43/103 [00:02<00:03, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 46/103 [00:02<00:03, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/103 [00:03<00:04, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/103 [00:03<00:03, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 70/103 [00:04<00:02, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 77/103 [00:04<00:01, 19.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 83/103 [00:04<00:00, 21.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 89/103 [00:05<00:00, 22.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 95/103 [00:05<00:00, 23.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 101/103 [00:05<00:00, 17.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n",
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 103/103 [00:05<00:00, 17.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n",
      "CPU times: user 5.99 s, sys: 22.3 ms, total: 6.01 s\n",
      "Wall time: 6.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<103x19 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 303 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we've already created the labels (saved in the database), we can load them in as a sparse matrix here too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 ms, sys: 0 ns, total: 6.19 ms\n",
      "Wall time: 5.83 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<103x19 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 303 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = labeler.load_matrix(session, split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the returned matrix is a special subclass of the `scipy.sparse.csr_matrix` class, with some special features which we demonstrate below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trend(Span(\"b'advances'\", sentence=786, chars=[163,170], words=[22,22]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelKey (LF_entity)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.get_key(session, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view statistics about the resulting label matrix.\n",
    "\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a *conflicting* non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_entity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_up_down</th>\n",
       "      <td>1</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.145631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_consecutives</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.038835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_helping_kw</th>\n",
       "      <td>3</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.145631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_decline</th>\n",
       "      <td>4</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.077670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temporal_signs</th>\n",
       "      <td>5</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.174757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_percent_dependant</th>\n",
       "      <td>6</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.038835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_since_during</th>\n",
       "      <td>7</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.087379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_dependency</th>\n",
       "      <td>8</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.077670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_cause_effect</th>\n",
       "      <td>9</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.048544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_explicit_change</th>\n",
       "      <td>10</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.126214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_continue</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temporal_patterns</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_value_kw</th>\n",
       "      <td>13</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.203883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_extrema</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_Person_subj_obj</th>\n",
       "      <td>15</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_date_sign</th>\n",
       "      <td>16</td>\n",
       "      <td>0.456311</td>\n",
       "      <td>0.456311</td>\n",
       "      <td>0.456311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_linear_tense</th>\n",
       "      <td>17</td>\n",
       "      <td>0.048544</td>\n",
       "      <td>0.048544</td>\n",
       "      <td>0.048544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_indicator_signs</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902913</td>\n",
       "      <td>0.708738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j  Coverage  Overlaps  Conflicts\n",
       "LF_entity              0  0.029126  0.029126   0.019417\n",
       "LF_up_down             1  0.262136  0.262136   0.145631\n",
       "LF_consecutives        2  0.038835  0.038835   0.038835\n",
       "LF_helping_kw          3  0.145631  0.145631   0.145631\n",
       "LF_decline             4  0.087379  0.087379   0.077670\n",
       "LF_temporal_signs      5  0.174757  0.174757   0.174757\n",
       "LF_percent_dependant   6  0.038835  0.038835   0.038835\n",
       "LF_since_during        7  0.106796  0.106796   0.087379\n",
       "LF_dependency          8  0.077670  0.077670   0.077670\n",
       "LF_cause_effect        9  0.106796  0.106796   0.048544\n",
       "LF_explicit_change    10  0.126214  0.126214   0.126214\n",
       "LF_continue           11  0.000000  0.000000   0.000000\n",
       "LF_temporal_patterns  12  0.000000  0.000000   0.000000\n",
       "LF_value_kw           13  0.203883  0.203883   0.203883\n",
       "LF_extrema            14  0.000000  0.000000   0.000000\n",
       "LF_Person_subj_obj    15  0.038835  0.038835   0.019417\n",
       "LF_date_sign          16  0.456311  0.456311   0.456311\n",
       "LF_linear_tense       17  0.048544  0.048544   0.048544\n",
       "LF_indicator_signs    18  1.000000  0.902913   0.708738"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the Generative Model\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12326816,  0.26019816,  0.11504114,  0.15670405,  0.10124476,\n",
       "        0.12293512,  0.09370453,  0.12987184,  0.08388728,  0.11220833,\n",
       "        0.09537644,  0.0788662 ,  0.07797983,  0.10279405,  0.09405633,\n",
       "        0.06032445,  0.13194671,  0.07970935,  0.29284476])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the learned accuracy parameters, and other statistics about the LFs learned by the generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.562949</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.560753</td>\n",
       "      <td>0.369570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.6696</td>\n",
       "      <td>0.627631</td>\n",
       "      <td>0.423824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.564750</td>\n",
       "      <td>0.6695</td>\n",
       "      <td>0.565599</td>\n",
       "      <td>0.374575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.582702</td>\n",
       "      <td>0.6729</td>\n",
       "      <td>0.580828</td>\n",
       "      <td>0.390591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.538878</td>\n",
       "      <td>0.6649</td>\n",
       "      <td>0.538062</td>\n",
       "      <td>0.352352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.562715</td>\n",
       "      <td>0.6689</td>\n",
       "      <td>0.560671</td>\n",
       "      <td>0.368168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.547262</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>0.545951</td>\n",
       "      <td>0.360360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.559994</td>\n",
       "      <td>0.6684</td>\n",
       "      <td>0.565561</td>\n",
       "      <td>0.372172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.546096</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.542133</td>\n",
       "      <td>0.368368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.554498</td>\n",
       "      <td>0.6725</td>\n",
       "      <td>0.555955</td>\n",
       "      <td>0.371972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.544038</td>\n",
       "      <td>0.6608</td>\n",
       "      <td>0.543530</td>\n",
       "      <td>0.364965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.537539</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.542874</td>\n",
       "      <td>0.367568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.523838</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>0.524875</td>\n",
       "      <td>0.356957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.547999</td>\n",
       "      <td>0.6646</td>\n",
       "      <td>0.547669</td>\n",
       "      <td>0.369169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.543862</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.542158</td>\n",
       "      <td>0.368168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.530021</td>\n",
       "      <td>0.6662</td>\n",
       "      <td>0.529118</td>\n",
       "      <td>0.360160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.560024</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.555651</td>\n",
       "      <td>0.386787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.534269</td>\n",
       "      <td>0.6624</td>\n",
       "      <td>0.533477</td>\n",
       "      <td>0.346146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.641778</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.639549</td>\n",
       "      <td>0.431231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Coverage  Precision    Recall\n",
       "0   0.562949    0.6688   0.560753  0.369570\n",
       "1   0.629630    0.6696   0.627631  0.423824\n",
       "2   0.564750    0.6695   0.565599  0.374575\n",
       "3   0.582702    0.6729   0.580828  0.390591\n",
       "4   0.538878    0.6649   0.538062  0.352352\n",
       "5   0.562715    0.6689   0.560671  0.368168\n",
       "6   0.547262    0.6739   0.545951  0.360360\n",
       "7   0.559994    0.6684   0.565561  0.372172\n",
       "8   0.546096    0.6660   0.542133  0.368368\n",
       "9   0.554498    0.6725   0.555955  0.371972\n",
       "10  0.544038    0.6608   0.543530  0.364965\n",
       "11  0.537539    0.6673   0.542874  0.367568\n",
       "12  0.523838    0.6712   0.524875  0.356957\n",
       "13  0.547999    0.6646   0.547669  0.369169\n",
       "14  0.543862    0.6680   0.542158  0.368168\n",
       "15  0.530021    0.6662   0.529118  0.360160\n",
       "16  0.560024    0.6714   0.555651  0.386787\n",
       "17  0.534269    0.6624   0.533477  0.346146\n",
       "18  0.641778    0.6750   0.639549  0.431231"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set.  First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [00:00<00:00, 31.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [00:00<00:00, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [00:01<00:00, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n",
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'fin', 'Tense': 'past'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [00:01<00:00, 18.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{74: 98, 'VerbForm': 'inf'}\n",
      "{74: 98, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we get the score of the generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 22 | FN: 2\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Generative Model Performance\n",
    "\n",
    "At this point, we should be getting an F1 score of around 0.4 to 0.5 on the development set, which is pretty good!  However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!  \n",
    "\n",
    "In fact, it is probably somewhat _overfit_ to this set. However this is fine, since in the next tutorial, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a _blind_ test set (i.e. one we never looked at during development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Some Error Analysis\n",
    "\n",
    "At this point, we might want to look at some examples in one of the error buckets. For example, one of the false negatives that we did not correctly label as true mentions.  To do this, we can again just use the `Viewer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(fp, session)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sv.get_selected() if sv else list(fp.union(fn))[0]\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see the labels that the LFs gave to this candidate using simple ORM-enabled syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also now explore some of the additional functionalities of the `lf_stats` method for our dev set LF labels, `L_dev`: we can plug in the gold labels that we have, and the accuracies that our generative model has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for labeling functions with low coverage, our learned accuracies are closer to our prior of 70% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our training labels\n",
    "\n",
    "Finally, we'll save the `training_marginals`, which are our **probabilistic training labels**, so that we can use them in the next tutorial to train our end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part III, we'll use these probabilistic training labels to train a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-butt": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {
    "5c30a7935aa84f14b282753d32a7c2a8": {
     "views": [
      {
       "cell_index": 49
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
